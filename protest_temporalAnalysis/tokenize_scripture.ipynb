{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to Tokenize Scripture References in WBC Sermons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to combine the pieces of a scripture reference Chapter, Book, and Verse into a single Token in order to include in model as a feature or extract for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os, sys, imp\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "import string\n",
    "import datetime\n",
    "import imp\n",
    "import math\n",
    "import warnings\n",
    "import re, string\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Network Analysis\n",
    "import igraph\n",
    "import scipy.spatial.distance as ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('..') # Change to Top Level GIT Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('./prototype_python') # Add location of python prototype to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('./protest_temporalAnalysis/') # Add location of python prototype to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import semanticDensity as sd\n",
    "import temporal_methods as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Sample of News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloc = './data_dsicap/WBC/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(dataloc)\n",
    "files = [file for file in files if '.txt' in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a Way to Tokenize Scripture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawText=open(dataloc+files[0], encoding='latin1').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to pre-process scripture references (before Tokenization) to maintain the references as an enitiety\n",
    "# @Text - Raw text of a document\n",
    "# @Returns: Text as Sting\n",
    "def collect_scripture(rawText):\n",
    "    # Find all Scripture References\n",
    "    m = re.findall('\\d*[ ]*[a-zA-Z.]+ \\d+:\\d+[,\\-]*[ ]*\\d*', rawText)\n",
    "    # Remove Leading and Tailing Whitespace\n",
    "    m = [verse.strip() for verse in m]\n",
    "    # For each unique reference, replace with single string\n",
    "    for verse in list(set(m)):\n",
    "        newverse = '_'+verse.replace(\" \",\"_\")+'_'\n",
    "        print ('replaced '+verse+' with '+newverse)\n",
    "        rawText = rawText.replace(verse,newverse)\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced Jn. 16:2 with _Jn._16:2_\n",
      "replaced Luke 19:14 with _Luke_19:14_\n",
      "replaced John. 3:16 with _John._3:16_\n",
      "replaced Rev. 6:10 with _Rev._6:10_\n",
      "replaced Rev. 19:14-16 with _Rev._19:14-16_\n",
      "replaced Job 19:26, 27 with _Job_19:26,_27_\n",
      "replaced Psa. 2:1-5 with _Psa._2:1-5_\n",
      "replaced Prov. 1:24-27 with _Prov._1:24-27_\n",
      "replaced Psa. 94:20 with _Psa._94:20_\n",
      "replaced Luke. 18:7, 8 with _Luke._18:7,_8_\n",
      "replaced 2 Pet. 1:10 with _2_Pet._1:10_\n",
      "replaced 1 Jn. 3:2 with _1_Jn._3:2_\n",
      "replaced Acts 22:22 with _Acts_22:22_\n",
      "replaced Rev. 1:13-17 with _Rev._1:13-17_\n",
      "replaced Psa. 58:10, 11 with _Psa._58:10,_11_\n",
      "replaced Jn. 16:2, with _Jn._16:2,_\n",
      "replaced Jn. 3:16-21 with _Jn._3:16-21_\n",
      "replaced Heb. 13:8 with _Heb._13:8_\n"
     ]
    }
   ],
   "source": [
    "rawText = collect_scripture(rawText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenList=tokenizer.tokenize(rawText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: Going to need to modify the pre-processing steps to not strip the scripture references. Also need to get rid of empty tokens. Plus tokens that are just puncuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenList = tm.clean_text(tokenList)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [rel35]",
   "language": "python",
   "name": "Python [rel35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
