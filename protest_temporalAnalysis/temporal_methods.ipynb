{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods designed for use at the document level\n",
    "\n",
    "\n",
    "Python 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "import string\n",
    "import datetime\n",
    "import imp\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# os.chdir('../prototype_python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import semanticDensity as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_xml(file, key, columnTitles):\n",
    "    #create empty dictionary\n",
    "    productRow = {}\n",
    "\n",
    "    #create list that will store the single dataframe rows\n",
    "    rows = []\n",
    "\n",
    "    #read in file noting when tags begin or end\n",
    "    for event, element in ET.iterparse(file, events=(\"start\", \"end\")):\n",
    "\n",
    "    #get current product\n",
    "        if event == \"start\" and element.tag == key:\n",
    "            productRow = {} # dictionary on current row\n",
    "\n",
    "        for title in columnTitles: #for each tag in the xml\n",
    "            if event == \"end\" and element.tag == title: #if a <\\TITLE> is present, then add\n",
    "                productRow[title] = element.text #add to dictionary\n",
    "\n",
    "    #done adding values - now append\n",
    "        if event == \"end\" and element.tag == columnTitles[len(columnTitles)-1]:\n",
    "            rows.append(productRow) #append to list of rows\n",
    "\n",
    "    #create pandas dataframe with column names of tags\n",
    "    frame = pd.DataFrame.from_records(rows, columns=columnTitles)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermFreq(tokenList):\n",
    "    TF = {}\n",
    "\n",
    "    for word in tokenList:\n",
    "        # print(word)\n",
    "        if word in TF:\n",
    "            TF[word] += 1\n",
    "        else:\n",
    "            TF[word] = 1\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define function to create context vectors\n",
    "def contextVectors(tokenList,dsm,wordlist,k):\n",
    "    \n",
    "    #Define coOccurence dict\n",
    "    cvDict={}\n",
    "\n",
    "    for i in range(len(tokenList)):\n",
    "        targetword=tokenList[i] # Changed window to targetword for more clarity\n",
    "        \n",
    "        if targetword in wordlist:\n",
    "            # print(targetword)\n",
    "            #Adjust window to contain words k in front or k behind\n",
    "            lowerBound=max(0,i-k)\n",
    "            upperBound=min(len(tokenList),i+k)\n",
    "            cvList=tokenList[lowerBound:i]+tokenList[i+1:upperBound+1]\n",
    "    \n",
    "\n",
    "            if targetword not in cvDict.keys():\n",
    "                cvDict[targetword]={}\n",
    "\n",
    "            #Create context vector            \n",
    "            contextVector={}\n",
    "\n",
    "            for word in cvList:\n",
    "                if word in dsm.keys():\n",
    "                    \n",
    "                    for key in dsm[word].keys(): # Need to Catch for Words in Window, but not in DSM                 \n",
    "                        #Update context vector\n",
    "                        try:\n",
    "                            contextVector[key]=contextVector[key]+dsm[word][key]\n",
    "                        except: # What is this except for?\n",
    "                            contextVector[key]=dsm[word][key]\n",
    "                else:\n",
    "                    for i in range(0,len(dsm[list(dsm.keys())[0]])):\n",
    "                        contextVector[i] = 1\n",
    "\n",
    "            #Add context vector to cvDict\n",
    "            cvIndex=len(cvDict[targetword])+1\n",
    "            cvDict[targetword][cvIndex]=contextVector\n",
    "    \n",
    "    #Return context vector dictionary\n",
    "    return(cvDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize34(path2File, file):\n",
    "    tokens={}\n",
    "    # print(path2File+file)\n",
    "    #Clean raw text into token list\n",
    "    rawText=open(path2File+file, errors='ignore').read()\n",
    "    # rawText=unicode(rawText, \"utf-8\", errors=\"ignore\")\n",
    "    #Update for encoding issues            \n",
    "    # rawText=unicode(rawText, \"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenList=tokenizer.tokenize(rawText)\n",
    "    \n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(path2File, file):\n",
    "    tokens={}\n",
    "    # print(path2File+file)\n",
    "    #Clean raw text into token list\n",
    "    rawText=open(path2File+file).read()\n",
    "    rawText=unicode(rawText, \"utf-8\", errors=\"ignore\")\n",
    "    #Update for encoding issues            \n",
    "    # rawText=unicode(rawText, \"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenList=tokenizer.tokenize(rawText)\n",
    "    \n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define tokenize function\n",
    "def clean_text(tokenList):\n",
    "    \n",
    "    #Convert all text to lower case\n",
    "    tokenList=[word.lower() for word in tokenList]\n",
    "\n",
    "    #Remove punctuation\n",
    "    tokenList=[word for word in tokenList if word not in punctuation]\n",
    "    # tokenList=[\"\".join(c for c in word if c not in punctuation) for word in tokenList ]\n",
    "    tokenList = [word if word[len(word)-1] not in punctuation else word[0:len(word)-1] for word in tokenList] # Mod-ing to only remove puncutation if at the end of the word\n",
    "\n",
    "    #convert digits into NUM\n",
    "    #tokenList=[re.sub(\"\\d+\", \"NUM\", word) for word in tokenList] \n",
    "    tokenList = ['NUM' if word.isdigit() else word for word in tokenList] # Mod-ing so that only tokens that are entirely numbers get replaced with 'Num'\n",
    "\n",
    "    #Stem words\n",
    "    #tokenList=[stemmer.stem(word) for word in tokenList]\n",
    "    #stemStopwords=[stemmer.stem(word) for word in stopWords]\n",
    "\n",
    "    #Remove blanks\n",
    "    tokenList=[word for word in tokenList if word!= ' ']\n",
    "\n",
    "    #Remove stopwords\n",
    "#            stemStopwords.append(\"\")\n",
    "#            tokenList=[word for word in tokenList if word not in stemStopwords]\n",
    "\n",
    "    #Return tokens\n",
    "    return(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(tokenList):\n",
    "    count = 0\n",
    "    for token in tokenList:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_specific_words(tokenList, word):\n",
    "    count = 0\n",
    "    for token in tokenList:\n",
    "        if token == word:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define context vector simCountulation\n",
    "def averageCosine(cvDict,simCount):\n",
    "    cosineResults=[]\n",
    "    for searchWord in cvDict.keys():\n",
    "        if len(cvDict[searchWord])>1:\n",
    "            consinesim=np.zeros(simCount)\n",
    "            for i in range(simCount):\n",
    "                x=random.randrange(0, len(cvDict[searchWord]))\n",
    "                y=random.randrange(0, len(cvDict[searchWord]))\n",
    "\n",
    "                consinesim[i]=sd.get_cosine(cvDict[searchWord][x+1],cvDict[searchWord][y+1])\n",
    "            approx_avg_cosine=np.average(consinesim)\n",
    "            cosineResults.append([searchWord,approx_avg_cosine,len(cvDict[searchWord])])\n",
    "        else:\n",
    "            cosineResults.append([searchWord,-1,len(cvDict[searchWord])])\n",
    "    return cosineResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get sentimentWord dict and remove duplicates. Store in lists\n",
    "posFilePath='/refData/positive-words.txt'\n",
    "negFilePath='/refData/negative-words.txt'\n",
    "posWords=list(set(tokenize34('.',posFilePath)))\n",
    "negWords=list(set(tokenize34('.',negFilePath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentimentLookup(tokens):\n",
    "    fileSentiment=[]\n",
    "    #Get sentiment for each document\n",
    "    # for filename in tokens.keys():\n",
    "        \n",
    "    #initialize counters\n",
    "    wordCount=0.0\n",
    "    posCount=0.0\n",
    "    negCount=0.0\n",
    "\n",
    "    #Get counts\n",
    "    for token in tokens:\n",
    "        #Add to word count\n",
    "        wordCount=wordCount+1        \n",
    "\n",
    "        #Check if positive\n",
    "        if token in posWords:\n",
    "            posCount=posCount+1\n",
    "\n",
    "        #Check if negative\n",
    "        if token in negWords:\n",
    "            negCount=negCount+1\n",
    "\n",
    "    #Calculate percentages and append to list\n",
    "    posPer=posCount/wordCount\n",
    "    negPer=negCount/wordCount\n",
    "    fileSentiment.append([posPer,negPer])\n",
    "    \n",
    "    #Calculate average sub-group level word sentiment percent\n",
    "    #wordSentiment=np.mean(np.array(fileSentiment),axis=0)\n",
    "    \n",
    "    #Calculate sub-group level doc sentiment percent\n",
    "    #posDocCount=float(len([x for x in fileSentiment if x[0]>x[1]]))\n",
    "    #posDocPer=posDocCount/len(fileSentiment)\n",
    "    #negDocPer=1-posDocPer\n",
    "    #output=[wordSentiment[0],wordSentiment[1],posDocPer,negDocPer]   \n",
    "    \n",
    "    return(fileSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRawText(path2File, file):\n",
    "    tokens={}\n",
    "    # print(path2File+file)\n",
    "    #Clean raw text into token list\n",
    "    rawText=open(path2File+file, encoding='latin1').read()\n",
    "    #rawText=str(unicode(rawText, \"utf-8\", errors=\"ignore\"))\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Load Metadata from Files (Dorothy, WBC, and simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDorothyDaymetadata(dataloc):\n",
    "\n",
    "    files = os.listdir(dataloc)\n",
    "\n",
    "    files = [file for file in files if '.txt' in file]\n",
    "\n",
    "    fileData = pd.DataFrame(files, columns = ['fileName'])\n",
    "\n",
    "    firstlines = []\n",
    "    for file in fileData.fileName:\n",
    "        text = rawText=open(dataloc+\"/\"+file, encoding = 'latin1').read()\n",
    "        lineone = text[0:text[1:len(text)].find('\\n')+1]\n",
    "        lineone = re.sub('\\nThe Catholic Worker, ','',lineone)\n",
    "        firstlines.append(lineone)\n",
    "\n",
    "    fileData['firstline'] = firstlines\n",
    "\n",
    "    # Building Parsing for Dates in DD Files\n",
    "    date_est = []\n",
    "    for i in range(0,len(fileData)):\n",
    "        if fileData.loc[i,'firstline'][0] == '\\n':\n",
    "            # print(fileData.loc[i,'firstline'][0])\n",
    "            # print(\"Drop\")\n",
    "            date_est.append('unclear date')\n",
    "        else:\n",
    "            # print(fileData.loc[i,'firstline'][0])\n",
    "            # print(\"Keep\")\n",
    "            pieces = fileData.loc[i,'firstline'].split(\" \")\n",
    "            if len(pieces[1]) != 5:\n",
    "                date_est.append('unclear date')\n",
    "            else:\n",
    "                date_est.append(pieces[0]+', '+pieces[1][0:4])\n",
    "                # print(pieces)\n",
    "\n",
    "    fileData['date_est'] = date_est\n",
    "\n",
    "    datelist = []\n",
    "    for date in date_est:\n",
    "        try:\n",
    "            datelist.append(datetime.datetime.strptime(date, '%B, %Y'))\n",
    "        except ValueError:\n",
    "            temp = re.split('[-,/]',date)\n",
    "            temp = temp[0]+','+temp[len(temp)-1]\n",
    "            try: \n",
    "                temp = datetime.datetime.strptime(temp, '%B, %Y')\n",
    "                datelist.append(temp)\n",
    "            except ValueError:\n",
    "                try: \n",
    "                    temp = datetime.datetime.strptime(temp, '%b, %Y')\n",
    "                    datelist.append(temp)\n",
    "                except ValueError:\n",
    "                    datelist.append(\"unclear date\")\n",
    "\n",
    "    fileData['date_clean'] = datelist\n",
    "    \n",
    "    fileData[fileData.date_clean == 'unclear date'] = np.NaN\n",
    "    fileData = fileData.dropna()\n",
    "    fileData = fileData[fileData.date_clean > datetime.datetime(1900, 1, 1, 0, 0)]\n",
    "    del fileData['date_est']\n",
    "    del fileData['firstline']\n",
    "    fileData.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    fileData.sort_values(by='date_clean', ascending=True, inplace=True)\n",
    "    \n",
    "    return fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSimplemetadata(dataloc):\n",
    "    'Loads File Metadata when Date is 1st Item in FileName e.g. 2 March 2017'\n",
    "    files = os.listdir(dataloc)\n",
    "    files = [file for file in files if '.txt' in file]\n",
    "    fileData = pd.DataFrame(files, columns = ['fileName'])\n",
    "    temp = [file.split('_')[0] for file in files]\n",
    "    fileData['date'] = temp\n",
    "    # Convert Str Dates to Date-Time Objects\n",
    "    dates_clean = [datetime.datetime.strptime(date,'%d %B %Y') for date in fileData.date]\n",
    "    fileData['date_clean'] = dates_clean\n",
    "    fileData.sort_values(by='date_clean', ascending=True, inplace=True)\n",
    "    return fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWBCmetadata(dataloc):\n",
    "    files = os.listdir(dataloc)\n",
    "    files = [file for file in files if '.txt' in file]\n",
    "    fileData = pd.DataFrame(files, columns = ['fileName'])\n",
    "    temp = [file.split('_')[2][0:8] for file in files]\n",
    "    fileData['date'] = temp\n",
    "    # Convert Str Dates to Date-Time Objects\n",
    "    dates_clean = [datetime.datetime.strptime(date,'%Y%m%d') for date in fileData.date]\n",
    "    fileData['date_clean'] = dates_clean\n",
    "    fileData.sort_values(by='date_clean', ascending=True, inplace=True)\n",
    "    return fileData"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [rel35]",
   "language": "python",
   "name": "Python [rel35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
